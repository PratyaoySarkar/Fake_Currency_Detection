{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28be59cb",
   "metadata": {},
   "source": [
    "# Using MobileNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd3c94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary modules\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49476353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2 is commonly trained on 224x224 images\n",
    "height = 224\n",
    "width = 224\n",
    "\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(height, width, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "616e14a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading and Augmentation ---\n",
    "train_dir = \"dataset/training\"\n",
    "validation_dir = \"dataset/validation\"\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1f2e16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2774 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input, # Use MobileNetV2's preprocess_input\n",
    "    rotation_range=90,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(height, width),\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90640449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 592 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(height, width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "955ee58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_finetune_model(base_model, dropout, fc_layers, num_classes):\n",
    "    # Freeze the layers of the base model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add custom top layers for classification\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    for fc in fc_layers:\n",
    "        x = Dense(fc, activation='relu')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    finetune_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return finetune_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f106c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = ['Real', 'Fake']\n",
    "FC_Layers = [1024, 1024]\n",
    "dropout = 0.5\n",
    "\n",
    "finetune_model = build_finetune_model(\n",
    "    base_model,\n",
    "    dropout=dropout,\n",
    "    fc_layers=FC_Layers,\n",
    "    num_classes=len(class_list)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e111ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "num_train_images = 2774 # From your notebook's output\n",
    "\n",
    "# Callbacks for saving the best model and stopping early\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"MobileNetV2_model.h5\",\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    save_freq='epoch'\n",
    ")\n",
    "early = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=40,\n",
    "    verbose=1,\n",
    "    mode=\"auto\"\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "optimizer = optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "finetune_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8265922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PRATYAOY SARKAR\\FinalYrProject\\myenv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637ms/step - accuracy: 0.5816 - loss: 8.8861\n",
      "Epoch 1: val_accuracy improved from None to 0.75000, saving model to MobileNetV2_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 652ms/step - accuracy: 0.6233 - loss: 4.7603 - val_accuracy: 0.7500 - val_loss: 0.6321\n",
      "Epoch 2/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:17\u001b[0m 573ms/step - accuracy: 0.8750 - loss: 0.4806"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PRATYAOY SARKAR\\FinalYrProject\\myenv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.75000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8750 - loss: 0.4806 - val_accuracy: 0.7500 - val_loss: 0.6269\n",
      "Epoch 3/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556ms/step - accuracy: 0.6412 - loss: 0.6369\n",
      "Epoch 3: val_accuracy improved from 0.75000 to 0.87500, saving model to MobileNetV2_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 563ms/step - accuracy: 0.6612 - loss: 0.5998 - val_accuracy: 0.8750 - val_loss: 0.6134\n",
      "Epoch 4/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:24\u001b[0m 592ms/step - accuracy: 0.7500 - loss: 0.5239\n",
      "Epoch 4: val_accuracy did not improve from 0.87500\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7500 - loss: 0.5239 - val_accuracy: 0.8750 - val_loss: 0.6099\n",
      "Epoch 5/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558ms/step - accuracy: 0.7147 - loss: 0.5253\n",
      "Epoch 5: val_accuracy improved from 0.87500 to 1.00000, saving model to MobileNetV2_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 566ms/step - accuracy: 0.7202 - loss: 0.5255 - val_accuracy: 1.0000 - val_loss: 0.3769\n",
      "Epoch 6/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:14\u001b[0m 563ms/step - accuracy: 0.5000 - loss: 0.5482\n",
      "Epoch 6: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 983us/step - accuracy: 0.5000 - loss: 0.5482 - val_accuracy: 1.0000 - val_loss: 0.3725\n",
      "Epoch 7/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565ms/step - accuracy: 0.7094 - loss: 0.5245\n",
      "Epoch 7: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 566ms/step - accuracy: 0.7180 - loss: 0.5092 - val_accuracy: 0.6250 - val_loss: 0.6054\n",
      "Epoch 8/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:18\u001b[0m 574ms/step - accuracy: 0.7500 - loss: 0.3836\n",
      "Epoch 8: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 995us/step - accuracy: 0.7500 - loss: 0.3836 - val_accuracy: 0.6250 - val_loss: 0.5956\n",
      "Epoch 9/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556ms/step - accuracy: 0.7799 - loss: 0.4340\n",
      "Epoch 9: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 557ms/step - accuracy: 0.7672 - loss: 0.4487 - val_accuracy: 1.0000 - val_loss: 0.3781\n",
      "Epoch 10/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:10\u001b[0m 552ms/step - accuracy: 0.8750 - loss: 0.3787\n",
      "Epoch 10: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 976us/step - accuracy: 0.8750 - loss: 0.3787 - val_accuracy: 1.0000 - val_loss: 0.3648\n",
      "Epoch 11/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568ms/step - accuracy: 0.7786 - loss: 0.4326\n",
      "Epoch 11: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 569ms/step - accuracy: 0.7733 - loss: 0.4402 - val_accuracy: 1.0000 - val_loss: 0.3841\n",
      "Epoch 12/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:12\u001b[0m 557ms/step - accuracy: 0.8750 - loss: 0.2011\n",
      "Epoch 12: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 953us/step - accuracy: 0.8750 - loss: 0.2011 - val_accuracy: 1.0000 - val_loss: 0.3997\n",
      "Epoch 13/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562ms/step - accuracy: 0.7877 - loss: 0.4252\n",
      "Epoch 13: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 563ms/step - accuracy: 0.7878 - loss: 0.4289 - val_accuracy: 1.0000 - val_loss: 0.2278\n",
      "Epoch 14/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:11\u001b[0m 556ms/step - accuracy: 0.6250 - loss: 0.5380\n",
      "Epoch 14: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 947us/step - accuracy: 0.6250 - loss: 0.5380 - val_accuracy: 1.0000 - val_loss: 0.2287\n",
      "Epoch 15/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618ms/step - accuracy: 0.8049 - loss: 0.4033\n",
      "Epoch 15: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 619ms/step - accuracy: 0.7961 - loss: 0.4082 - val_accuracy: 1.0000 - val_loss: 0.3365\n",
      "Epoch 16/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:26\u001b[0m 599ms/step - accuracy: 1.0000 - loss: 0.1695\n",
      "Epoch 16: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.1695 - val_accuracy: 1.0000 - val_loss: 0.3365\n",
      "Epoch 17/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566ms/step - accuracy: 0.7966 - loss: 0.4197\n",
      "Epoch 17: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 567ms/step - accuracy: 0.7968 - loss: 0.4249 - val_accuracy: 0.8750 - val_loss: 0.3932\n",
      "Epoch 18/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:20\u001b[0m 582ms/step - accuracy: 0.6250 - loss: 1.2966\n",
      "Epoch 18: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6250 - loss: 1.2966 - val_accuracy: 0.8750 - val_loss: 0.3914\n",
      "Epoch 19/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556ms/step - accuracy: 0.7963 - loss: 0.3874\n",
      "Epoch 19: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 557ms/step - accuracy: 0.8004 - loss: 0.3901 - val_accuracy: 0.8750 - val_loss: 0.5232\n",
      "Epoch 20/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:14\u001b[0m 564ms/step - accuracy: 0.8750 - loss: 0.4355\n",
      "Epoch 20: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - accuracy: 0.8750 - loss: 0.4355 - val_accuracy: 0.8750 - val_loss: 0.5226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a6b1befd10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=num_train_images // batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=1, # As in the original notebook\n",
    "    callbacks=[checkpoint, early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48454e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 304ms/step\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"MobileNetV2_model.h5\" \n",
    "\n",
    "model = load_model(MODEL_PATH)\n",
    "true_labels = validation_generator.classes\n",
    "class_labels = list(validation_generator.class_indices.keys())\n",
    "\n",
    "predictions = model.predict(validation_generator)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1f97ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d1557f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Evaluation Report ---\n",
      "Accuracy: 0.6402\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.60      0.84      0.70       296\n",
      "        real       0.74      0.44      0.55       296\n",
      "\n",
      "    accuracy                           0.64       592\n",
      "   macro avg       0.67      0.64      0.62       592\n",
      "weighted avg       0.67      0.64      0.62       592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Model Evaluation Report ---\")\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "# Print precision, recall, f1-score for each class\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=class_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
