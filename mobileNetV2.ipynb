{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e268609",
   "metadata": {},
   "source": [
    "# Updation of previous model with Adaptive Moment Estimation (Adam) optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e59582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam # Import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau # Import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ebc871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2 is commonly trained on 224x224 images\n",
    "height = 224\n",
    "width = 224\n",
    "\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(height, width, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e448644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading and Augmentation ---\n",
    "train_dir = \"dataset/training\"\n",
    "validation_dir = \"dataset/validation\"\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "467a18b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2774 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input, # Use MobileNetV2's preprocess_input\n",
    "    rotation_range=90,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(height, width),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f778a23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 592 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(height, width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8e56f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_finetune_model(base_model, dropout, fc_layers, num_classes):\n",
    "    # Freeze the layers of the base model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add custom top layers for classification\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    for fc in fc_layers:\n",
    "        x = Dense(fc, activation='relu')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    finetune_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return finetune_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b85e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = ['Real', 'Fake']\n",
    "FC_Layers = [1024, 1024]\n",
    "dropout = 0.5\n",
    "\n",
    "finetune_model = build_finetune_model(\n",
    "    base_model,\n",
    "    dropout=dropout,\n",
    "    fc_layers=FC_Layers,\n",
    "    num_classes=len(class_list)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "266e3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "num_train_images = 2774\n",
    "num_validation_images = 592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c158b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving the best model and stopping early\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"MobileNetV2_model_improved.h5\", # Using a new name for the improved model\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    save_freq='epoch'\n",
    ")\n",
    "early = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=10, # Can be reduced as the scheduler will help\n",
    "    verbose=1,\n",
    "    mode=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ddf496",
   "metadata": {},
   "source": [
    "# We are using ReduceLROnPlateau, which will reduce the learning rate when a metric (validation_accuracy) has stopped improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e12422ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "# Compile the model with the Adam optimizer\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "finetune_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, early, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "454d169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PRATYAOY SARKAR\\FinalYrProject\\myenv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6607 - loss: 19.3735\n",
      "Epoch 1: val_accuracy improved from None to 0.68074, saving model to MobileNetV2_model_improved.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 1s/step - accuracy: 0.7061 - loss: 10.4164 - val_accuracy: 0.6807 - val_loss: 0.8353 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:30\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.8468"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PRATYAOY SARKAR\\FinalYrProject\\myenv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.68074\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.8468 - val_accuracy: 0.6723 - val_loss: 0.7964 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7351 - loss: 0.7932\n",
      "Epoch 3: val_accuracy improved from 0.68074 to 0.75676, saving model to MobileNetV2_model_improved.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 1s/step - accuracy: 0.7339 - loss: 0.7082 - val_accuracy: 0.7568 - val_loss: 0.5253 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:26\u001b[0m 1s/step - accuracy: 0.8750 - loss: 0.3415\n",
      "Epoch 4: val_accuracy did not improve from 0.75676\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 53ms/step - accuracy: 0.8750 - loss: 0.3415 - val_accuracy: 0.7534 - val_loss: 0.5254 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7503 - loss: 0.6399\n",
      "Epoch 5: val_accuracy improved from 0.75676 to 0.76182, saving model to MobileNetV2_model_improved.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 1s/step - accuracy: 0.7393 - loss: 0.6390 - val_accuracy: 0.7618 - val_loss: 0.6166 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:18\u001b[0m 1s/step - accuracy: 0.7500 - loss: 0.2781\n",
      "Epoch 6: val_accuracy improved from 0.76182 to 0.79223, saving model to MobileNetV2_model_improved.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 63ms/step - accuracy: 0.7500 - loss: 0.2781 - val_accuracy: 0.7922 - val_loss: 0.5454 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7178 - loss: 0.8049\n",
      "Epoch 7: val_accuracy did not improve from 0.79223\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 1s/step - accuracy: 0.7209 - loss: 0.6881 - val_accuracy: 0.7382 - val_loss: 0.5866 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:35\u001b[0m 1s/step - accuracy: 0.8750 - loss: 0.2385\n",
      "Epoch 8: val_accuracy did not improve from 0.79223\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 65ms/step - accuracy: 0.8750 - loss: 0.2385 - val_accuracy: 0.7365 - val_loss: 0.5829 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7410 - loss: 0.5165\n",
      "Epoch 9: val_accuracy did not improve from 0.79223\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 1s/step - accuracy: 0.7621 - loss: 0.4773 - val_accuracy: 0.7483 - val_loss: 0.5356 - learning_rate: 2.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:54\u001b[0m 1s/step - accuracy: 0.8750 - loss: 0.3168\n",
      "Epoch 10: val_accuracy did not improve from 0.79223\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.8750 - loss: 0.3168 - val_accuracy: 0.7483 - val_loss: 0.5380 - learning_rate: 2.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7907 - loss: 0.4203\n",
      "Epoch 11: val_accuracy did not improve from 0.79223\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 1s/step - accuracy: 0.7820 - loss: 0.4234 - val_accuracy: 0.7416 - val_loss: 0.5156 - learning_rate: 2.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:11\u001b[0m 1s/step - accuracy: 0.6250 - loss: 0.4471\n",
      "Epoch 12: val_accuracy did not improve from 0.79223\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6250 - loss: 0.4471 - val_accuracy: 0.7449 - val_loss: 0.5124 - learning_rate: 2.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8083 - loss: 0.3735\n",
      "Epoch 13: val_accuracy did not improve from 0.79223\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 1s/step - accuracy: 0.8026 - loss: 0.3948 - val_accuracy: 0.7922 - val_loss: 0.5006 - learning_rate: 2.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:11\u001b[0m 1s/step - accuracy: 0.7500 - loss: 0.2788\n",
      "Epoch 14: val_accuracy improved from 0.79223 to 0.79561, saving model to MobileNetV2_model_improved.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.2788 - val_accuracy: 0.7956 - val_loss: 0.4997 - learning_rate: 2.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7969 - loss: 0.4066\n",
      "Epoch 15: val_accuracy improved from 0.79561 to 0.81926, saving model to MobileNetV2_model_improved.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 1s/step - accuracy: 0.8069 - loss: 0.4043 - val_accuracy: 0.8193 - val_loss: 0.4651 - learning_rate: 2.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:11\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.3438\n",
      "Epoch 16: val_accuracy did not improve from 0.81926\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.3438 - val_accuracy: 0.8193 - val_loss: 0.4676 - learning_rate: 2.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8267 - loss: 0.3698\n",
      "Epoch 17: val_accuracy did not improve from 0.81926\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 1s/step - accuracy: 0.8196 - loss: 0.3810 - val_accuracy: 0.7770 - val_loss: 0.4786 - learning_rate: 2.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:04\u001b[0m 1s/step - accuracy: 0.8750 - loss: 0.4454\n",
      "Epoch 18: val_accuracy did not improve from 0.81926\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.8750 - loss: 0.4454 - val_accuracy: 0.7703 - val_loss: 0.4795 - learning_rate: 2.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8208 - loss: 0.3531\n",
      "Epoch 19: val_accuracy did not improve from 0.81926\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 1s/step - accuracy: 0.8228 - loss: 0.3617 - val_accuracy: 0.7770 - val_loss: 0.4532 - learning_rate: 2.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m  1/346\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:03\u001b[0m 1s/step - accuracy: 0.7500 - loss: 0.4699\n",
      "Epoch 20: val_accuracy did not improve from 0.81926\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.7500 - loss: 0.4699 - val_accuracy: 0.7804 - val_loss: 0.4509 - learning_rate: 2.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f301d36dd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=num_train_images // batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=num_validation_images // batch_size, # Corrected validation_steps\n",
    "    callbacks=callbacks # Use the new callbacks list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "532e7ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 250ms/step\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"MobileNetV2_model_improved.h5\" \n",
    "\n",
    "model = load_model(MODEL_PATH)\n",
    "true_labels = validation_generator.classes\n",
    "class_labels = list(validation_generator.class_indices.keys())\n",
    "\n",
    "predictions = model.predict(validation_generator)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d8a95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe4c20a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Evaluation Report ---\n",
      "Accuracy: 0.8193\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.78      0.89      0.83       296\n",
      "        real       0.87      0.75      0.81       296\n",
      "\n",
      "    accuracy                           0.82       592\n",
      "   macro avg       0.83      0.82      0.82       592\n",
      "weighted avg       0.83      0.82      0.82       592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Model Evaluation Report ---\")\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "# Print precision, recall, f1-score for each class\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681de6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
